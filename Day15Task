Exercise 1: Understanding RDD and Partitioning
Objective: Create and manipulate an RDD while understanding its partitions.
Task:

Load a large text file (or create one programmatically with millions of random numbers).
Perform the following:
Check the number of partitions for the RDD.
Repartition the RDD into 4 partitions and analyze how the data is distributed.
Coalesce the RDD back into 2 partitions.
Print the first 5 elements from each partition.
Expected Analysis:

View the effect of repartition and coalesce in the Spark UI (stages, tasks).

===========================================================================

Exercise 2: Narrow vs Wide Transformations
Objective: Differentiate between narrow and wide transformations in Spark.
Task:

Create an RDD of numbers from 1 to 1000.
Apply narrow transformations: map, filter.
Apply a wide transformation: groupByKey or reduceByKey (simulate by mapping numbers into key-value pairs, e.g., (number % 10, number)).
Save the results to a text file.
Expected Analysis:

Identify how narrow transformations execute within a single partition, while wide transformations cause shuffles.
Observe the DAG in the Spark UI, focusing on stages and shuffle operations.

=================================================================================
Exercise 3: Analyzing Tasks and Executors
Objective: Understand how tasks are distributed across executors in local mode.
Task:

Create an RDD of strings with at least 1 million lines (e.g., lorem ipsum or repetitive text).
Perform a transformation pipeline:
Split each string into words.
Map each word to (word, 1).
Reduce by key to count word occurrences.
Set spark.executor.instances to 2 and observe task distribution in the Spark UI.
Expected Analysis:

Compare task execution times across partitions and stages in the UI.
Understand executor and task allocation for a local mode Spark job.

====================================================================================
Exercise 4: Exploring DAG and Spark UI
Objective: Analyze the DAG and understand the stages involved in a complex Spark job.
Task:

Create an RDD of integers from 1 to 10,000.
Perform a series of transformations:

Filter: Keep only even numbers.
Map: Multiply each number by 10.
Map: Generate a tuple where the first element is the remainder when dividing the number by 100 (key), and the second is the number itself (value).
ReduceByKey: Group by the remainder (key) and compute the sum of the values.
Finally, perform an action to collect the results and display them.
Expected Analysis:

Analyze the DAG generated for the job and how Spark breaks it into stages.
Compare execution times of stages and tasks in the Spark UI.

=================================================================================
Exercise 5: Partitioning Impact on Performance
Objective: Understand the impact of partitioning on performance and data shuffling.
Task:

Load a large dataset (e.g., a CSV or JSON file) into an RDD.
Partition the RDD into 2, 4, and 8 partitions separately and perform the following tasks:
Count the number of rows in the RDD.
Sort the data using a wide transformation.
Write the output back to disk.
Compare execution times for different partition sizes.
Expected Analysis:

Observe how partition sizes affect shuffle size and task distribution in the Spark UI.
Understand the trade-off between too many and too few partitions.
